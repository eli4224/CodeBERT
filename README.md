# Code Pretraining Models

This repo contains the code for [Delayed Complete Self-Attention and Improvements to
Positional Encodings for GraphCodeBERT](https://drive.google.com/file/d/1xF9YK9XARC5czf4zt19l9lHgeujMRV-c/view?usp=sharing), a reimplimentation of the MLM pretraining task for the GraphCodeBERT model from Microsoft([GraphCodeBERT: Pre-training Code Representations with Data Flow](https://openreview.net/pdf?id=jLoC4ez43PZ)) with additional experiments and improvements.

It's based heavily on the code and data of the Code Search fine-tuning task released by Microsoft alongside their paper.

## Contact

Feel free to contact me if you have any further questions [eli4224graphcodebert@gmail.com](mailto:eli4224graphcodebert@gmail.com). 

## Contributing

If you have any suggestions or improvements, feel free to contribute! Open an issue or a pull request.
